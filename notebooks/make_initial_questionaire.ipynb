{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Delphi Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/1_question.md') as f:\n",
    "    question = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "os.environ['OPENAI_API_KEY'] = config['openai_api_key']\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview') ## make this defined ones and used again and again!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Key Themes\n",
    "\n",
    "Tinker with this persona a bit more - it does work well. But it's crafted as if it wants other people to do the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/determine_key_themes.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"can you provide key themes around the question or subject of '{question}'\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "key_themes = llm.invoke(chat_template.format_messages(\n",
    "    question=question)).content\n",
    "\n",
    "with open(\"outputs/2_key_themes.md\", \"w\") as text_file:\n",
    "    text_file.write(key_themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Initial Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/create_the_initial_questionnaire.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"can you questionnaire to gather information around the question or subject of '{question}' \n",
    "            making sure to take into account the key themes from Dr. Renn, here are his findings: {themes}\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI() ## make this defined ones and used again and again!!\n",
    "questionnaire = llm.invoke(chat_template.format_messages(\n",
    "    question=question, themes = key_themes)).content\n",
    "\n",
    "with open(\"outputs/3_questionnaire.md\", \"w\") as text_file:\n",
    "    text_file.write(questionnaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Initial Questionnaire into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/turn_initial_questionnaire_to_JSON.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Can you convert the following into a JSON object: {questionnaire}\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI() ## make this defined ones and used again and again!!\n",
    "questionaire_json = llm.invoke(chat_template.format_messages(\n",
    "    questionnaire=questionnaire)).content\n",
    "\n",
    "\n",
    "questionnaire_json = json.loads(questionaire_json)\n",
    "with open(\"outputs/3_questionnaire.json\", \"w\") as f:\n",
    "    json.dump(questionnaire_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

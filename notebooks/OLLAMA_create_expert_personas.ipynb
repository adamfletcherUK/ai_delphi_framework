{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Expert Personas to Answer Initial Questionaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# with open('config.yaml', 'r') as file:\n",
    "#     config = yaml.safe_load(file)\n",
    "# os.environ['OPENAI_API_KEY'] = config['openai_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/1_question.md\", \"r\") as f:\n",
    "    question = f.read()\n",
    "\n",
    "with open(\"outputs/2_key_themes.md\", \"r\") as f:\n",
    "    key_themes = f.read()\n",
    "\n",
    "with open(\"outputs/3_questionnaire.md\", \"r\") as f:\n",
    "    questionare = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an Initial Overview of Expert Types\n",
    "\n",
    "\n",
    "the types of experts we might need -- give a high level overview of the experts we might need.\n",
    "This is needed as ChatGPT cannot make all detailed personas at once - we need to upramp the amount of text made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/make_initial_overview_of_expert_types.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"can you craft descriptions of the different type of experts which would make up a diverse panel of experts on the subject of '{question}', with the key themes being '{key_themes}' where the questions being answered are '{questionare}'\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOllama(model='mistral') ## make this defined ones and used again and again!!\n",
    "expert_personas_overview_md = llm.invoke(chat_template.format_messages(\n",
    "    question=question, key_themes=key_themes, questionare=questionare)).content\n",
    "\n",
    "with open(\"outputs/OLLAMA_4_expert_personas_overview.md\", \"w\") as text_file:\n",
    "    text_file.write(expert_personas_overview_md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Initial Overview to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/convert_initial_overview_to_JSON.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Can you convert the following into a JSON object: {expert_personas_overview_md}\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI() ## make this defined ones and used again and again!!\n",
    "expert_personas_overview_json = llm.invoke(chat_template.format_messages(\n",
    "    expert_personas_overview_md=expert_personas_overview_md)).content\n",
    "\n",
    "\n",
    "persona_overview_json = json.loads(expert_personas_overview_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Craft Expert Persona Descriptions from Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/craft_persona_from_overview_persona.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Can you convert craft various LLM personas for the role of {role}, with the characteristics of {characteristics}. \n",
    "            Which could suitably answer the question of '{question}', where the key themes are '{key_themes}' and the questions being asked are '{questionare}'\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model='gpt-4-turbo-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Protection and Compliance Expert\n",
      "Scalability and Performance Expert\n",
      "Machine Learning Continuous Improvement Expert\n",
      "Diverse Email Request Recognition Expert\n",
      "Seamless Integration Expert\n",
      "Regulatory Compliance and Governance Expert\n",
      "Cost-Benefit Analysis Expert\n",
      "Collaboration and Stakeholder Engagement Expert\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('outputs/OLLAMA_5_custom_personas/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "for i in persona_overview_json['Expert Personas']:\n",
    "    print(i['Role'])\n",
    "    specific_personas = llm.invoke(chat_template.format_messages(\n",
    "    role=i['Role'], \n",
    "    characteristics=i['characteristics'], \n",
    "    question=question, key_themes=key_themes, questionare=questionare)).content\n",
    "\n",
    "    with open(f\"outputs/OLLAMA_5_custom_personas/{i['Role']}.txt\", \"w\") as text_file:\n",
    "        text_file.write(specific_personas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Expert Persona Descriptions to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diverse Email Request Recognition Expert\n",
      "Regulatory Compliance and Governance Expert\n",
      "Data Protection and Compliance Expert\n",
      "Seamless Integration Expert\n",
      "Machine Learning Continuous Improvement Expert\n",
      "Collaboration and Stakeholder Engagement Expert\n",
      "Cost-Benefit Analysis Expert\n",
      "Scalability and Performance Expert\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/convert_expert_persona_descriptions_to_JSON.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Can you convert the following into a JSON object: {specific_personas}\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# with open(\"outputs/specific_personas.txt\", \"r\") as text_file:\n",
    "#     specific_personas = text_file.read()\n",
    "\n",
    "# llm = ChatOpenAI(model='gpt-4-turbo-preview') ## make this defined ones and used again and again!!\n",
    "\n",
    "\n",
    "files = glob.glob('outputs/OLLAMA_5_custom_personas/*.txt')\n",
    "for file in files:\n",
    "    print(os.path.basename(file).replace('.txt',''))\n",
    "    with open(file, 'r') as text_file:\n",
    "        specific_personas = text_file.read()\n",
    "\n",
    "\n",
    "    specific_personas_json = llm.invoke(chat_template.format_messages(\n",
    "        specific_personas=specific_personas)).content\n",
    "\n",
    "\n",
    "    specific_personas_json = json.loads(specific_personas_json)\n",
    "    with open(f\"outputs/OLLAMA_5_custom_personas/{os.path.basename(file).replace('.txt','')}.json\", 'w') as f:\n",
    "        json.dump(specific_personas_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesh Out Expert Personas with Viewpoints\n",
    "\n",
    "This step adds different viewpoints, beliefs and behavioural characteristsics to personas in order to get different perspectives on the questionnaire with the same expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(open('./persona_descriptions/flesh_out_expert_personas_with_viewpoints.txt').read())\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"Can you convert craft a detailed personas for the role of {role}, with the characteristics of {characteristics}. \n",
    "            Which could suitably answer questions where the key themes are '{key_themes}'\"\"\"\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model='gpt-4-turbo-preview') ## make this defined ones and used again and again!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diverse Email Request Recognition Expert\n",
      "  -  Machine Learning Security & Compliance Specialist\n",
      "  -  Machine Learning Efficiency & Scalability Engineer\n",
      "  -  Machine Learning Ethics & Fairness Expert\n",
      "  -  Machine Learning Integration & Deployment Engineer\n",
      "  -  Machine Learning Data Scientist & Engineer\n",
      "  -  Machine Learning Governance & Compliance Officer\n",
      "Cost-Benefit Analysis Expert\n",
      "  -  Data Security & Privacy Expert\n",
      "  -  Scalability & High Performance Expert\n",
      "  -  Machine Learning Specialist\n",
      "  -  Regulatory Compliance Expert\n",
      "  -  Ethical Use & Interface Design Expert\n",
      "Collaboration and Stakeholder Engagement Expert\n",
      "  -  Data Security & Compliance Specialist\n",
      "  -  Machine Learning Engineer & Data Anonymization Expert\n",
      "  -  DevOps & Infrastructure Specialist\n",
      "  -  Ethics & Bias Specialist\n",
      "  -  Regulatory Compliance & Governance Expert\n",
      "Regulatory Compliance and Governance Expert\n",
      "  -  Security & Compliance Specialist\n",
      "  -  Machine Learning Engineer\n",
      "  -  Scalability & Performance Engineer\n",
      "  -  Feedback & Collaboration Specialist\n",
      "  -  Ethics & Transparency Specialist\n",
      "Scalability and Performance Expert\n",
      "  -  Data Protection & Compliance Expert\n",
      "  -  Scalability & Machine Learning Optimization Expert\n",
      "  -  Model Integration & Deployment Expert\n",
      "  -  Model Fairness & Ethics Expert\n",
      "  -  Governance & Compliance Specialist\n",
      "  -  Cost Analysis & ROI Specialist\n",
      "  -  Cloud vs. On-Premise Specialist\n",
      "  -  Stakeholder Engagement & Collaboration Expert\n",
      "Seamless Integration Expert\n",
      "  -  Data Protection Regulations Compliance Expert\n",
      "  -  Scalability, Performance and Capacity Expert\n",
      "  -  Machine Learning Model Training Expert\n",
      "  -  Ethical Considerations Expert\n",
      "  -  Seamless Integration and User Experience Expert\n",
      "  -  Regulatory Compliance and Governance Structures Expert\n",
      "  -  Cost Evaluation and Savings Expert\n",
      "  -  Machine Learning Frameworks, Tools, and Languages Expert\n",
      "  -  Cloud vs. On-Premise Deployment Choices Expert\n",
      "  -  Stakeholder Engagement and Collaboration Strategies Expert\n",
      "  -  Ethical Considerations and User Feedback Mechanisms Expert\n",
      "  -  Cost Evaluation and ROI Expert\n",
      "  -  Selection of Tools and Frameworks Expert\n",
      "Data Protection and Compliance Expert\n",
      "  -  Security & Compliance Specialist\n",
      "  -  Machine Learning Engineer\n",
      "  -  Data Scientist\n",
      "  -  DevOps Engineer\n",
      "  -  Ethics & Compliance Officer\n",
      "Machine Learning Continuous Improvement Expert\n",
      "  -  Security & Compliance Engineer\n",
      "  -  Scalability & Performance Engineer\n",
      "  -  Machine Learning Engineer\n",
      "  -  Bias Reduction & Ethics Engineer\n",
      "  -  Integration & Deployment Engineer\n",
      "  -  Cloud vs. On-premise Engineer\n",
      "  -  Collaboration & Governance Engineer\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('outputs/OLLAMA_6_final_personas/*.md')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "files = glob.glob('outputs/OLLAMA_5_custom_personas/*.json')\n",
    "for file in files:\n",
    "    print(os.path.basename(file).replace('.json',''))\n",
    "    with open(file, 'r') as j:\n",
    "        specific_personas_json = json.loads(j.read())\n",
    "\n",
    "    for i in specific_personas_json['Expert Personas']:\n",
    "        print(f\"  -  {i['Role']}\")\n",
    "        detailed_persona = llm.invoke(chat_template.format_messages(\n",
    "            role=i['Role'], \n",
    "            characteristics=i['characteristics'], \n",
    "            question=question, key_themes=key_themes, questionare=questionare)).content\n",
    "\n",
    "        with open(f\"outputs/OLLAMA_6_final_personas/{os.path.basename(file).replace('.json','')} {i['Role'].replace(':', '')}.md\", \"w\") as text_file:\n",
    "            text_file.write(detailed_persona)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
